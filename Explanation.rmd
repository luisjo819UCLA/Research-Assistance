---
title: "RA Luis Zapata - PHD Luca - Anderson School of Economics"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Explanation of Code

I will be utilizing matched employer-employee data from Germany. In this [link](https://fdz.iab.de/en/int_bd_pd/the-sample-of-integrated-employer-employee-data-sieed-sieed-7518-version-1/), you can find a description of the dataset. Specifically, there is a file at the link containing a detailed description of the data ([this](https://doku.iab.de/fdz/reporte/2020/DR_14-20_EN.pdf)) and the test data, which is a fake data. Unfortunately, access to the real data is restricted due to confidentiality constraints associated with the dataset.

This [paper](https://davidcard.berkeley.edu/papers/QJE-2013.pdf) is an excellent piece that utilizes the same dataset, implementing certain methodologies and approaches that I aim to replicate or use as a foundation for my analysis. Additionally, the paper discusses some issues with the dataset, including the fact that wages are capped. When wages exceed a specific threshold, they are recorded as the threshold value. The authors employ a methodology to estimate the actual wage beyond the threshold, and I plan to adopt a similar approach. The initial task will involve replicating their code, translating it into either R. 

## Preparing the Data

We beggin by calling the libraries as follow:

```{r, message = FALSE,warning = FALSE}
library(data.table) #Table data handler
library(censReg)  # For Tobit regression
library(dplyr) #Also data handler
library(tidyverse,quietly = TRUE) #For data managment
library(lubridate)  # For easy date handling
library(haven) # For reading Stata files
```

## Call the data

We call the Stata data (.dta) from our folder and store it in a list. 
```{r}
dta_files = list.files("SIEED_7518_v1_test", pattern = ".dta")
dta_list = lapply(dta_files, function(x) read_dta(paste0("SIEED_7518_v1_test/", x)))
print(class(dta_list))
```

I now print the last 10 rows of the list to see what kind of data we are dealing with. 
```{r}
print(tail(dta_files, 10))
```

We can see that the data have one kind related with years, while the others have the names: basis, entry, exit, infow and outflow associated with them.
 

```{r}
#On dta_files there are data with year on the name, and other without it.
#Lets filter the ones with year on the name

dta_files_2 <- dta_files[str_detect(dta_files, "^(SIEED_7518_v1_bhp_20|SIEED_7518_v1_bhp_19)")]

dta_files_3 <- dta_files[!dta_files %in% dta_files_2]

print(dta_files_3)
```
We know merge all the data that are related with the yeards 2019 and 2020. 

```{r}
dta_year = tibble("Address" = dta_files_2) %>%
  mutate("Year" = str_sub(Address, start = 19, end = 22))
#We extract the year, wich is the second four digits of the file name
#Lets read all the dta in the Address column and store them in a list. Lets name the list with the year of the data
dta_list_2 = lapply(dta_files_2, function(x) read_dta(paste0("SIEED_7518_v1_test/", x)))
names(dta_list_2) = dta_year$Year
#Lets merge all the data in dta_list_2, in order to create one big tibble. We also create one column with name "Year" to store the year of the data
dta_list_2 = dta_list_2 %>%
  bind_rows(.id = "Year") %>%
  select(-Year)
print(paste("The size of the data is: rows:", nrow(dta_list_2), " and columns:", ncol(dta_list_2)))
print(head(dta_list_2,3))
```


```{r}
print(glimpse(dta_list_2))
```

We still have to understand better the way the data is gathered. We have to read the documentation in order to do it.

```{r}
print(dta_files_3)
```